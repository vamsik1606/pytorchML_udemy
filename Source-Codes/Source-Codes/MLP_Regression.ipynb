{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston"
      ],
      "metadata": {
        "id": "PCloQdyjFLAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "df['target'] = boston.target\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K4UafXpRF0Js",
        "outputId": "63c39c98-2ec6-4ac6-f002-fc5f7afbeba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
              "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
              "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
              "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
              "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
              "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
              "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
              "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
              "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
              "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
              "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
              "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
              "\n",
              "     PTRATIO       B  LSTAT  target  \n",
              "0       15.3  396.90   4.98    24.0  \n",
              "1       17.8  396.90   9.14    21.6  \n",
              "2       17.8  392.83   4.03    34.7  \n",
              "3       18.7  394.63   2.94    33.4  \n",
              "4       18.7  396.90   5.33    36.2  \n",
              "..       ...     ...    ...     ...  \n",
              "501     21.0  391.99   9.67    22.4  \n",
              "502     21.0  396.90   9.08    20.6  \n",
              "503     21.0  396.90   5.64    23.9  \n",
              "504     21.0  393.45   6.48    22.0  \n",
              "505     21.0  396.90   7.88    11.9  \n",
              "\n",
              "[506 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc5ccdfd-1d4f-486c-8f77-e2a10b3a538c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "      <td>22.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "      <td>11.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc5ccdfd-1d4f-486c-8f77-e2a10b3a538c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc5ccdfd-1d4f-486c-8f77-e2a10b3a538c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc5ccdfd-1d4f-486c-8f77-e2a10b3a538c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(boston.DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iX8zrdZG10g",
        "outputId": "1b291ff6-f59d-44e1-8574-bc83452fa3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = boston.data, boston.target"
      ],
      "metadata": {
        "id": "d_JKTo8yHMA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
      ],
      "metadata": {
        "id": "1rg0fDrpHWAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train).float()\n",
        "X_test = torch.tensor(X_test).float()\n",
        "y_train = torch.tensor(y_train).float().unsqueeze(1)\n",
        "y_test = torch.tensor(y_test).float().unsqueeze(1)   "
      ],
      "metadata": {
        "id": "L5a_5AWSHzfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = X_train.mean(dim=0)\n",
        "std = X_train.std(dim=0)\n",
        "X_train = (X_train-mean)/std\n",
        "X_test = (X_test-mean)/std"
      ],
      "metadata": {
        "id": "uand3AAiJDvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression Model**"
      ],
      "metadata": {
        "id": "b4M2nq56MAiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(13,128,bias = True)  # Define the hidden layer\n",
        "    self.out = nn.Linear(128,1,bias = True)  # Define the output layer\n",
        "    self.relu = nn.ReLU()        # Define the activation function\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "wWrar4sRMQRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression(input_size = X_train.shape[1],output_size=1)\n",
        "#X_train.shape[1]"
      ],
      "metadata": {
        "id": "6xPxyR4TQoVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()  # nn.L1Loss\n",
        "optimizer = optim.SGD(model.parameters(),lr = 0.01)"
      ],
      "metadata": {
        "id": "n4HusE6yRFuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "kc37Xs1J1k41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "num_epochs = 100000\n",
        "for epoch in range(num_epochs):\n",
        "  #Forward pass\n",
        "  y_pred = model(X_train)\n",
        "  loss = criterion(y_pred,y_train)\n",
        "\n",
        "  #Backward Pass\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print(f'Epoch [{epoch + 1}/100000], Loss: {loss.item(): .4f}')\n",
        "  losses.append(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovRmbf42_ejd",
        "outputId": "b077f5f1-f229-493c-bfa3-f9458ab7cb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/100000], Loss:  10.8245\n",
            "Epoch [200/100000], Loss:  8.9074\n",
            "Epoch [300/100000], Loss:  7.9844\n",
            "Epoch [400/100000], Loss:  7.3795\n",
            "Epoch [500/100000], Loss:  6.8915\n",
            "Epoch [600/100000], Loss:  6.4802\n",
            "Epoch [700/100000], Loss:  6.1539\n",
            "Epoch [800/100000], Loss:  5.8784\n",
            "Epoch [900/100000], Loss:  5.6758\n",
            "Epoch [1000/100000], Loss:  5.5831\n",
            "Epoch [1100/100000], Loss:  5.5222\n",
            "Epoch [1200/100000], Loss:  5.3414\n",
            "Epoch [1300/100000], Loss:  5.2709\n",
            "Epoch [1400/100000], Loss:  5.3000\n",
            "Epoch [1500/100000], Loss:  5.2720\n",
            "Epoch [1600/100000], Loss:  5.0143\n",
            "Epoch [1700/100000], Loss:  4.6494\n",
            "Epoch [1800/100000], Loss:  5.0292\n",
            "Epoch [1900/100000], Loss:  4.2802\n",
            "Epoch [2000/100000], Loss:  5.0172\n",
            "Epoch [2100/100000], Loss:  4.1010\n",
            "Epoch [2200/100000], Loss:  4.0645\n",
            "Epoch [2300/100000], Loss:  4.8087\n",
            "Epoch [2400/100000], Loss:  3.6396\n",
            "Epoch [2500/100000], Loss:  4.3462\n",
            "Epoch [2600/100000], Loss:  4.0568\n",
            "Epoch [2700/100000], Loss:  3.3936\n",
            "Epoch [2800/100000], Loss:  4.2899\n",
            "Epoch [2900/100000], Loss:  4.0185\n",
            "Epoch [3000/100000], Loss:  3.1880\n",
            "Epoch [3100/100000], Loss:  3.4198\n",
            "Epoch [3200/100000], Loss:  4.2662\n",
            "Epoch [3300/100000], Loss:  3.3380\n",
            "Epoch [3400/100000], Loss:  3.0419\n",
            "Epoch [3500/100000], Loss:  4.0676\n",
            "Epoch [3600/100000], Loss:  3.2532\n",
            "Epoch [3700/100000], Loss:  2.7786\n",
            "Epoch [3800/100000], Loss:  3.6616\n",
            "Epoch [3900/100000], Loss:  3.9938\n",
            "Epoch [4000/100000], Loss:  2.9474\n",
            "Epoch [4100/100000], Loss:  2.5138\n",
            "Epoch [4200/100000], Loss:  2.6013\n",
            "Epoch [4300/100000], Loss:  3.5613\n",
            "Epoch [4400/100000], Loss:  3.6196\n",
            "Epoch [4500/100000], Loss:  2.5768\n",
            "Epoch [4600/100000], Loss:  2.3457\n",
            "Epoch [4700/100000], Loss:  2.7019\n",
            "Epoch [4800/100000], Loss:  3.6502\n",
            "Epoch [4900/100000], Loss:  3.4367\n",
            "Epoch [5000/100000], Loss:  2.4698\n",
            "Epoch [5100/100000], Loss:  2.1813\n",
            "Epoch [5200/100000], Loss:  2.3273\n",
            "Epoch [5300/100000], Loss:  2.7861\n",
            "Epoch [5400/100000], Loss:  3.4852\n",
            "Epoch [5500/100000], Loss:  3.6094\n",
            "Epoch [5600/100000], Loss:  2.7612\n",
            "Epoch [5700/100000], Loss:  2.3126\n",
            "Epoch [5800/100000], Loss:  2.0854\n",
            "Epoch [5900/100000], Loss:  1.9593\n",
            "Epoch [6000/100000], Loss:  1.8913\n",
            "Epoch [6100/100000], Loss:  1.8441\n",
            "Epoch [6200/100000], Loss:  1.8077\n",
            "Epoch [6300/100000], Loss:  1.7666\n",
            "Epoch [6400/100000], Loss:  1.7502\n",
            "Epoch [6500/100000], Loss:  1.7407\n",
            "Epoch [6600/100000], Loss:  1.7373\n",
            "Epoch [6700/100000], Loss:  1.7298\n",
            "Epoch [6800/100000], Loss:  1.7045\n",
            "Epoch [6900/100000], Loss:  1.6977\n",
            "Epoch [7000/100000], Loss:  1.6939\n",
            "Epoch [7100/100000], Loss:  1.6674\n",
            "Epoch [7200/100000], Loss:  1.6557\n",
            "Epoch [7300/100000], Loss:  1.6444\n",
            "Epoch [7400/100000], Loss:  1.6434\n",
            "Epoch [7500/100000], Loss:  1.6312\n",
            "Epoch [7600/100000], Loss:  1.6344\n",
            "Epoch [7700/100000], Loss:  1.6360\n",
            "Epoch [7800/100000], Loss:  1.6110\n",
            "Epoch [7900/100000], Loss:  1.5922\n",
            "Epoch [8000/100000], Loss:  1.5892\n",
            "Epoch [8100/100000], Loss:  1.6017\n",
            "Epoch [8200/100000], Loss:  1.6036\n",
            "Epoch [8300/100000], Loss:  1.6059\n",
            "Epoch [8400/100000], Loss:  1.6107\n",
            "Epoch [8500/100000], Loss:  1.6182\n",
            "Epoch [8600/100000], Loss:  1.6469\n",
            "Epoch [8700/100000], Loss:  1.6868\n",
            "Epoch [8800/100000], Loss:  1.7238\n",
            "Epoch [8900/100000], Loss:  1.7940\n",
            "Epoch [9000/100000], Loss:  1.8934\n",
            "Epoch [9100/100000], Loss:  2.0725\n",
            "Epoch [9200/100000], Loss:  2.3224\n",
            "Epoch [9300/100000], Loss:  2.5614\n",
            "Epoch [9400/100000], Loss:  2.5522\n",
            "Epoch [9500/100000], Loss:  2.0798\n",
            "Epoch [9600/100000], Loss:  1.7339\n",
            "Epoch [9700/100000], Loss:  1.5612\n",
            "Epoch [9800/100000], Loss:  1.5095\n",
            "Epoch [9900/100000], Loss:  1.6228\n",
            "Epoch [10000/100000], Loss:  1.9139\n",
            "Epoch [10100/100000], Loss:  2.4548\n",
            "Epoch [10200/100000], Loss:  2.6286\n",
            "Epoch [10300/100000], Loss:  2.2820\n",
            "Epoch [10400/100000], Loss:  1.8299\n",
            "Epoch [10500/100000], Loss:  1.5552\n",
            "Epoch [10600/100000], Loss:  1.4259\n",
            "Epoch [10700/100000], Loss:  1.3822\n",
            "Epoch [10800/100000], Loss:  1.4262\n",
            "Epoch [10900/100000], Loss:  1.5073\n",
            "Epoch [11000/100000], Loss:  1.6523\n",
            "Epoch [11100/100000], Loss:  1.8085\n",
            "Epoch [11200/100000], Loss:  1.9736\n",
            "Epoch [11300/100000], Loss:  2.1355\n",
            "Epoch [11400/100000], Loss:  2.3251\n",
            "Epoch [11500/100000], Loss:  2.5013\n",
            "Epoch [11600/100000], Loss:  2.6075\n",
            "Epoch [11700/100000], Loss:  2.5376\n",
            "Epoch [11800/100000], Loss:  2.3969\n",
            "Epoch [11900/100000], Loss:  2.2603\n",
            "Epoch [12000/100000], Loss:  2.0731\n",
            "Epoch [12100/100000], Loss:  1.8924\n",
            "Epoch [12200/100000], Loss:  1.7854\n",
            "Epoch [12300/100000], Loss:  1.7015\n",
            "Epoch [12400/100000], Loss:  1.6258\n",
            "Epoch [12500/100000], Loss:  1.6528\n",
            "Epoch [12600/100000], Loss:  1.6708\n",
            "Epoch [12700/100000], Loss:  1.7533\n",
            "Epoch [12800/100000], Loss:  1.9003\n",
            "Epoch [12900/100000], Loss:  2.1021\n",
            "Epoch [13000/100000], Loss:  2.3809\n",
            "Epoch [13100/100000], Loss:  2.5195\n",
            "Epoch [13200/100000], Loss:  2.4520\n",
            "Epoch [13300/100000], Loss:  2.2771\n",
            "Epoch [13400/100000], Loss:  1.9690\n",
            "Epoch [13500/100000], Loss:  1.7341\n",
            "Epoch [13600/100000], Loss:  1.5144\n",
            "Epoch [13700/100000], Loss:  1.2949\n",
            "Epoch [13800/100000], Loss:  1.1591\n",
            "Epoch [13900/100000], Loss:  1.0828\n",
            "Epoch [14000/100000], Loss:  1.0480\n",
            "Epoch [14100/100000], Loss:  1.0671\n",
            "Epoch [14200/100000], Loss:  1.1251\n",
            "Epoch [14300/100000], Loss:  1.2584\n",
            "Epoch [14400/100000], Loss:  1.6267\n",
            "Epoch [14500/100000], Loss:  2.0705\n",
            "Epoch [14600/100000], Loss:  2.4148\n",
            "Epoch [14700/100000], Loss:  2.3576\n",
            "Epoch [14800/100000], Loss:  1.8983\n",
            "Epoch [14900/100000], Loss:  1.4154\n",
            "Epoch [15000/100000], Loss:  1.0617\n",
            "Epoch [15100/100000], Loss:  0.9257\n",
            "Epoch [15200/100000], Loss:  1.1692\n",
            "Epoch [15300/100000], Loss:  2.3495\n",
            "Epoch [15400/100000], Loss:  1.9183\n",
            "Epoch [15500/100000], Loss:  1.2904\n",
            "Epoch [15600/100000], Loss:  1.2860\n",
            "Epoch [15700/100000], Loss:  1.4315\n",
            "Epoch [15800/100000], Loss:  0.9379\n",
            "Epoch [15900/100000], Loss:  1.5203\n",
            "Epoch [16000/100000], Loss:  1.5372\n",
            "Epoch [16100/100000], Loss:  1.6060\n",
            "Epoch [16200/100000], Loss:  2.2452\n",
            "Epoch [16300/100000], Loss:  1.9981\n",
            "Epoch [16400/100000], Loss:  1.2557\n",
            "Epoch [16500/100000], Loss:  1.1019\n",
            "Epoch [16600/100000], Loss:  1.7893\n",
            "Epoch [16700/100000], Loss:  1.5211\n",
            "Epoch [16800/100000], Loss:  1.6620\n",
            "Epoch [16900/100000], Loss:  1.6820\n",
            "Epoch [17000/100000], Loss:  1.9866\n",
            "Epoch [17100/100000], Loss:  1.6082\n",
            "Epoch [17200/100000], Loss:  1.1204\n",
            "Epoch [17300/100000], Loss:  0.9196\n",
            "Epoch [17400/100000], Loss:  0.9219\n",
            "Epoch [17500/100000], Loss:  1.3324\n",
            "Epoch [17600/100000], Loss:  2.1342\n",
            "Epoch [17700/100000], Loss:  1.3470\n",
            "Epoch [17800/100000], Loss:  1.1695\n",
            "Epoch [17900/100000], Loss:  4.0075\n",
            "Epoch [18000/100000], Loss:  1.1428\n",
            "Epoch [18100/100000], Loss:  1.5892\n",
            "Epoch [18200/100000], Loss:  4.4379\n",
            "Epoch [18300/100000], Loss:  1.2599\n",
            "Epoch [18400/100000], Loss:  1.2999\n",
            "Epoch [18500/100000], Loss:  1.4024\n",
            "Epoch [18600/100000], Loss:  1.2557\n",
            "Epoch [18700/100000], Loss:  0.9938\n",
            "Epoch [18800/100000], Loss:  0.8450\n",
            "Epoch [18900/100000], Loss:  0.9398\n",
            "Epoch [19000/100000], Loss:  2.0831\n",
            "Epoch [19100/100000], Loss:  1.4764\n",
            "Epoch [19200/100000], Loss:  0.9657\n",
            "Epoch [19300/100000], Loss:  0.7956\n",
            "Epoch [19400/100000], Loss:  0.9431\n",
            "Epoch [19500/100000], Loss:  1.4158\n",
            "Epoch [19600/100000], Loss:  1.1380\n",
            "Epoch [19700/100000], Loss:  0.9000\n",
            "Epoch [19800/100000], Loss:  1.5468\n",
            "Epoch [19900/100000], Loss:  1.7886\n",
            "Epoch [20000/100000], Loss:  1.6206\n",
            "Epoch [20100/100000], Loss:  1.0499\n",
            "Epoch [20200/100000], Loss:  0.8435\n",
            "Epoch [20300/100000], Loss:  0.8374\n",
            "Epoch [20400/100000], Loss:  1.1797\n",
            "Epoch [20500/100000], Loss:  1.5867\n",
            "Epoch [20600/100000], Loss:  1.3986\n",
            "Epoch [20700/100000], Loss:  1.2120\n",
            "Epoch [20800/100000], Loss:  1.7446\n",
            "Epoch [20900/100000], Loss:  0.6957\n",
            "Epoch [21000/100000], Loss:  0.9719\n",
            "Epoch [21100/100000], Loss:  0.9406\n",
            "Epoch [21200/100000], Loss:  1.1972\n",
            "Epoch [21300/100000], Loss:  2.0179\n",
            "Epoch [21400/100000], Loss:  1.0364\n",
            "Epoch [21500/100000], Loss:  0.7269\n",
            "Epoch [21600/100000], Loss:  0.9592\n",
            "Epoch [21700/100000], Loss:  1.7514\n",
            "Epoch [21800/100000], Loss:  0.8443\n",
            "Epoch [21900/100000], Loss:  0.7563\n",
            "Epoch [22000/100000], Loss:  1.3549\n",
            "Epoch [22100/100000], Loss:  1.1142\n",
            "Epoch [22200/100000], Loss:  1.3652\n",
            "Epoch [22300/100000], Loss:  0.6519\n",
            "Epoch [22400/100000], Loss:  0.8915\n",
            "Epoch [22500/100000], Loss:  1.5434\n",
            "Epoch [22600/100000], Loss:  1.3193\n",
            "Epoch [22700/100000], Loss:  0.7631\n",
            "Epoch [22800/100000], Loss:  0.7264\n",
            "Epoch [22900/100000], Loss:  1.5137\n",
            "Epoch [23000/100000], Loss:  1.4978\n",
            "Epoch [23100/100000], Loss:  1.0599\n",
            "Epoch [23200/100000], Loss:  0.8165\n",
            "Epoch [23300/100000], Loss:  1.2956\n",
            "Epoch [23400/100000], Loss:  0.9455\n",
            "Epoch [23500/100000], Loss:  0.7334\n",
            "Epoch [23600/100000], Loss:  2.1335\n",
            "Epoch [23700/100000], Loss:  2.0034\n",
            "Epoch [23800/100000], Loss:  0.6380\n",
            "Epoch [23900/100000], Loss:  0.8212\n",
            "Epoch [24000/100000], Loss:  0.9860\n",
            "Epoch [24100/100000], Loss:  1.8673\n",
            "Epoch [24200/100000], Loss:  0.9654\n",
            "Epoch [24300/100000], Loss:  0.6625\n",
            "Epoch [24400/100000], Loss:  1.2127\n",
            "Epoch [24500/100000], Loss:  1.4390\n",
            "Epoch [24600/100000], Loss:  0.9047\n",
            "Epoch [24700/100000], Loss:  1.5394\n",
            "Epoch [24800/100000], Loss:  3.2008\n",
            "Epoch [24900/100000], Loss:  0.7636\n",
            "Epoch [25000/100000], Loss:  0.7534\n",
            "Epoch [25100/100000], Loss:  0.8050\n",
            "Epoch [25200/100000], Loss:  1.2598\n",
            "Epoch [25300/100000], Loss:  1.3446\n",
            "Epoch [25400/100000], Loss:  0.7548\n",
            "Epoch [25500/100000], Loss:  0.7402\n",
            "Epoch [25600/100000], Loss:  1.3146\n",
            "Epoch [25700/100000], Loss:  1.0706\n",
            "Epoch [25800/100000], Loss:  1.7672\n",
            "Epoch [25900/100000], Loss:  0.8547\n",
            "Epoch [26000/100000], Loss:  0.7513\n",
            "Epoch [26100/100000], Loss:  0.7615\n",
            "Epoch [26200/100000], Loss:  1.2131\n",
            "Epoch [26300/100000], Loss:  1.2826\n",
            "Epoch [26400/100000], Loss:  0.8506\n",
            "Epoch [26500/100000], Loss:  1.6295\n",
            "Epoch [26600/100000], Loss:  3.1277\n",
            "Epoch [26700/100000], Loss:  0.6559\n",
            "Epoch [26800/100000], Loss:  0.7079\n",
            "Epoch [26900/100000], Loss:  0.8964\n",
            "Epoch [27000/100000], Loss:  1.4751\n",
            "Epoch [27100/100000], Loss:  0.9578\n",
            "Epoch [27200/100000], Loss:  0.6481\n",
            "Epoch [27300/100000], Loss:  1.0684\n",
            "Epoch [27400/100000], Loss:  1.3126\n",
            "Epoch [27500/100000], Loss:  0.7149\n",
            "Epoch [27600/100000], Loss:  0.9673\n",
            "Epoch [27700/100000], Loss:  1.1155\n",
            "Epoch [27800/100000], Loss:  1.0824\n",
            "Epoch [27900/100000], Loss:  0.5160\n",
            "Epoch [28000/100000], Loss:  0.8677\n",
            "Epoch [28100/100000], Loss:  1.0447\n",
            "Epoch [28200/100000], Loss:  1.2056\n",
            "Epoch [28300/100000], Loss:  0.9940\n",
            "Epoch [28400/100000], Loss:  0.6506\n",
            "Epoch [28500/100000], Loss:  0.6553\n",
            "Epoch [28600/100000], Loss:  1.8719\n",
            "Epoch [28700/100000], Loss:  0.9390\n",
            "Epoch [28800/100000], Loss:  2.2669\n",
            "Epoch [28900/100000], Loss:  0.6585\n",
            "Epoch [29000/100000], Loss:  0.6839\n",
            "Epoch [29100/100000], Loss:  0.7628\n",
            "Epoch [29200/100000], Loss:  1.0174\n",
            "Epoch [29300/100000], Loss:  1.2312\n",
            "Epoch [29400/100000], Loss:  0.7769\n",
            "Epoch [29500/100000], Loss:  0.5584\n",
            "Epoch [29600/100000], Loss:  0.9702\n",
            "Epoch [29700/100000], Loss:  2.6223\n",
            "Epoch [29800/100000], Loss:  0.8406\n",
            "Epoch [29900/100000], Loss:  0.5044\n",
            "Epoch [30000/100000], Loss:  1.0686\n",
            "Epoch [30100/100000], Loss:  0.7960\n",
            "Epoch [30200/100000], Loss:  0.6675\n",
            "Epoch [30300/100000], Loss:  0.6840\n",
            "Epoch [30400/100000], Loss:  1.2734\n",
            "Epoch [30500/100000], Loss:  0.8507\n",
            "Epoch [30600/100000], Loss:  0.5231\n",
            "Epoch [30700/100000], Loss:  1.1154\n",
            "Epoch [30800/100000], Loss:  2.5351\n",
            "Epoch [30900/100000], Loss:  0.8162\n",
            "Epoch [31000/100000], Loss:  0.4658\n",
            "Epoch [31100/100000], Loss:  1.0916\n",
            "Epoch [31200/100000], Loss:  0.8441\n",
            "Epoch [31300/100000], Loss:  0.6596\n",
            "Epoch [31400/100000], Loss:  0.5881\n",
            "Epoch [31500/100000], Loss:  0.9279\n",
            "Epoch [31600/100000], Loss:  1.8953\n",
            "Epoch [31700/100000], Loss:  0.9278\n",
            "Epoch [31800/100000], Loss:  0.4284\n",
            "Epoch [31900/100000], Loss:  1.0002\n",
            "Epoch [32000/100000], Loss:  1.0005\n",
            "Epoch [32100/100000], Loss:  0.8952\n",
            "Epoch [32200/100000], Loss:  0.6727\n",
            "Epoch [32300/100000], Loss:  0.6370\n",
            "Epoch [32400/100000], Loss:  1.1931\n",
            "Epoch [32500/100000], Loss:  0.9105\n",
            "Epoch [32600/100000], Loss:  0.7720\n",
            "Epoch [32700/100000], Loss:  1.3604\n",
            "Epoch [32800/100000], Loss:  2.9657\n",
            "Epoch [32900/100000], Loss:  0.4909\n",
            "Epoch [33000/100000], Loss:  0.7017\n",
            "Epoch [33100/100000], Loss:  0.8482\n",
            "Epoch [33200/100000], Loss:  1.1783\n",
            "Epoch [33300/100000], Loss:  0.8596\n",
            "Epoch [33400/100000], Loss:  0.5244\n",
            "Epoch [33500/100000], Loss:  0.7670\n",
            "Epoch [33600/100000], Loss:  1.2923\n",
            "Epoch [33700/100000], Loss:  0.9324\n",
            "Epoch [33800/100000], Loss:  0.9964\n",
            "Epoch [33900/100000], Loss:  0.7077\n",
            "Epoch [34000/100000], Loss:  0.6551\n",
            "Epoch [34100/100000], Loss:  0.6328\n",
            "Epoch [34200/100000], Loss:  0.7241\n",
            "Epoch [34300/100000], Loss:  1.1930\n",
            "Epoch [34400/100000], Loss:  0.6624\n",
            "Epoch [34500/100000], Loss:  0.5544\n",
            "Epoch [34600/100000], Loss:  2.2346\n",
            "Epoch [34700/100000], Loss:  0.7785\n",
            "Epoch [34800/100000], Loss:  0.4227\n",
            "Epoch [34900/100000], Loss:  0.9778\n",
            "Epoch [35000/100000], Loss:  0.7888\n",
            "Epoch [35100/100000], Loss:  0.6236\n",
            "Epoch [35200/100000], Loss:  0.5489\n",
            "Epoch [35300/100000], Loss:  1.2809\n",
            "Epoch [35400/100000], Loss:  0.8261\n",
            "Epoch [35500/100000], Loss:  1.1507\n",
            "Epoch [35600/100000], Loss:  2.2114\n",
            "Epoch [35700/100000], Loss:  0.4408\n",
            "Epoch [35800/100000], Loss:  0.6790\n",
            "Epoch [35900/100000], Loss:  0.7849\n",
            "Epoch [36000/100000], Loss:  0.8799\n",
            "Epoch [36100/100000], Loss:  1.0610\n",
            "Epoch [36200/100000], Loss:  0.8685\n",
            "Epoch [36300/100000], Loss:  1.3877\n",
            "Epoch [36400/100000], Loss:  1.3217\n",
            "Epoch [36500/100000], Loss:  0.4054\n",
            "Epoch [36600/100000], Loss:  0.7426\n",
            "Epoch [36700/100000], Loss:  0.9339\n",
            "Epoch [36800/100000], Loss:  0.9858\n",
            "Epoch [36900/100000], Loss:  0.7668\n",
            "Epoch [37000/100000], Loss:  0.5529\n",
            "Epoch [37100/100000], Loss:  1.2803\n",
            "Epoch [37200/100000], Loss:  0.8836\n",
            "Epoch [37300/100000], Loss:  1.2312\n",
            "Epoch [37400/100000], Loss:  0.5408\n",
            "Epoch [37500/100000], Loss:  0.5772\n",
            "Epoch [37600/100000], Loss:  0.6084\n",
            "Epoch [37700/100000], Loss:  0.6287\n",
            "Epoch [37800/100000], Loss:  0.9521\n",
            "Epoch [37900/100000], Loss:  0.9075\n",
            "Epoch [38000/100000], Loss:  0.8160\n",
            "Epoch [38100/100000], Loss:  1.0224\n",
            "Epoch [38200/100000], Loss:  1.5391\n",
            "Epoch [38300/100000], Loss:  0.3994\n",
            "Epoch [38400/100000], Loss:  0.6511\n",
            "Epoch [38500/100000], Loss:  0.7886\n",
            "Epoch [38600/100000], Loss:  1.0458\n",
            "Epoch [38700/100000], Loss:  0.7504\n",
            "Epoch [38800/100000], Loss:  0.4654\n",
            "Epoch [38900/100000], Loss:  0.6574\n",
            "Epoch [39000/100000], Loss:  2.0459\n",
            "Epoch [39100/100000], Loss:  0.6998\n",
            "Epoch [39200/100000], Loss:  0.4530\n",
            "Epoch [39300/100000], Loss:  0.7823\n",
            "Epoch [39400/100000], Loss:  0.6451\n",
            "Epoch [39500/100000], Loss:  0.6033\n",
            "Epoch [39600/100000], Loss:  0.5597\n",
            "Epoch [39700/100000], Loss:  0.9596\n",
            "Epoch [39800/100000], Loss:  0.7053\n",
            "Epoch [39900/100000], Loss:  0.8435\n",
            "Epoch [40000/100000], Loss:  0.9995\n",
            "Epoch [40100/100000], Loss:  0.7671\n",
            "Epoch [40200/100000], Loss:  0.3593\n",
            "Epoch [40300/100000], Loss:  0.8824\n",
            "Epoch [40400/100000], Loss:  0.4453\n",
            "Epoch [40500/100000], Loss:  0.6128\n",
            "Epoch [40600/100000], Loss:  1.5815\n",
            "Epoch [40700/100000], Loss:  0.7904\n",
            "Epoch [40800/100000], Loss:  3.5963\n",
            "Epoch [40900/100000], Loss:  0.3821\n",
            "Epoch [41000/100000], Loss:  0.6614\n",
            "Epoch [41100/100000], Loss:  0.8116\n",
            "Epoch [41200/100000], Loss:  0.8273\n",
            "Epoch [41300/100000], Loss:  0.5666\n",
            "Epoch [41400/100000], Loss:  0.5185\n",
            "Epoch [41500/100000], Loss:  0.9474\n",
            "Epoch [41600/100000], Loss:  0.7954\n",
            "Epoch [41700/100000], Loss:  1.0972\n",
            "Epoch [41800/100000], Loss:  0.6534\n",
            "Epoch [41900/100000], Loss:  0.3437\n",
            "Epoch [42000/100000], Loss:  0.7105\n",
            "Epoch [42100/100000], Loss:  0.5220\n",
            "Epoch [42200/100000], Loss:  0.4951\n",
            "Epoch [42300/100000], Loss:  0.9731\n",
            "Epoch [42400/100000], Loss:  0.6746\n",
            "Epoch [42500/100000], Loss:  0.4054\n",
            "Epoch [42600/100000], Loss:  1.9527\n",
            "Epoch [42700/100000], Loss:  0.7194\n",
            "Epoch [42800/100000], Loss:  2.9526\n",
            "Epoch [42900/100000], Loss:  0.3742\n",
            "Epoch [43000/100000], Loss:  0.7098\n",
            "Epoch [43100/100000], Loss:  0.8374\n",
            "Epoch [43200/100000], Loss:  0.5414\n",
            "Epoch [43300/100000], Loss:  0.6980\n",
            "Epoch [43400/100000], Loss:  0.8453\n",
            "Epoch [43500/100000], Loss:  0.5715\n",
            "Epoch [43600/100000], Loss:  1.4729\n",
            "Epoch [43700/100000], Loss:  0.6778\n",
            "Epoch [43800/100000], Loss:  0.3275\n",
            "Epoch [43900/100000], Loss:  0.8440\n",
            "Epoch [44000/100000], Loss:  0.5926\n",
            "Epoch [44100/100000], Loss:  0.4286\n",
            "Epoch [44200/100000], Loss:  1.1146\n",
            "Epoch [44300/100000], Loss:  0.4979\n",
            "Epoch [44400/100000], Loss:  0.3858\n",
            "Epoch [44500/100000], Loss:  1.7519\n",
            "Epoch [44600/100000], Loss:  0.6865\n",
            "Epoch [44700/100000], Loss:  1.0175\n",
            "Epoch [44800/100000], Loss:  0.4717\n",
            "Epoch [44900/100000], Loss:  0.4854\n",
            "Epoch [45000/100000], Loss:  0.6271\n",
            "Epoch [45100/100000], Loss:  0.8471\n",
            "Epoch [45200/100000], Loss:  0.3769\n",
            "Epoch [45300/100000], Loss:  0.7037\n",
            "Epoch [45400/100000], Loss:  0.7540\n",
            "Epoch [45500/100000], Loss:  1.0710\n",
            "Epoch [45600/100000], Loss:  0.7580\n",
            "Epoch [45700/100000], Loss:  0.3132\n",
            "Epoch [45800/100000], Loss:  0.6916\n",
            "Epoch [45900/100000], Loss:  0.7107\n",
            "Epoch [46000/100000], Loss:  0.5790\n",
            "Epoch [46100/100000], Loss:  0.4069\n",
            "Epoch [46200/100000], Loss:  1.6253\n",
            "Epoch [46300/100000], Loss:  0.6566\n",
            "Epoch [46400/100000], Loss:  0.4267\n",
            "Epoch [46500/100000], Loss:  0.5552\n",
            "Epoch [46600/100000], Loss:  0.4794\n",
            "Epoch [46700/100000], Loss:  0.4410\n",
            "Epoch [46800/100000], Loss:  0.8075\n",
            "Epoch [46900/100000], Loss:  0.6007\n",
            "Epoch [47000/100000], Loss:  0.3840\n",
            "Epoch [47100/100000], Loss:  0.8778\n",
            "Epoch [47200/100000], Loss:  0.5844\n",
            "Epoch [47300/100000], Loss:  1.3660\n",
            "Epoch [47400/100000], Loss:  0.5922\n",
            "Epoch [47500/100000], Loss:  0.3206\n",
            "Epoch [47600/100000], Loss:  0.6167\n",
            "Epoch [47700/100000], Loss:  0.4446\n",
            "Epoch [47800/100000], Loss:  0.4014\n",
            "Epoch [47900/100000], Loss:  0.9930\n",
            "Epoch [48000/100000], Loss:  0.6007\n",
            "Epoch [48100/100000], Loss:  1.0962\n",
            "Epoch [48200/100000], Loss:  0.6426\n",
            "Epoch [48300/100000], Loss:  0.2951\n",
            "Epoch [48400/100000], Loss:  0.7293\n",
            "Epoch [48500/100000], Loss:  0.5723\n",
            "Epoch [48600/100000], Loss:  0.4304\n",
            "Epoch [48700/100000], Loss:  0.4522\n",
            "Epoch [48800/100000], Loss:  1.0968\n",
            "Epoch [48900/100000], Loss:  0.7804\n",
            "Epoch [49000/100000], Loss:  2.7324\n",
            "Epoch [49100/100000], Loss:  0.3251\n",
            "Epoch [49200/100000], Loss:  0.5078\n",
            "Epoch [49300/100000], Loss:  0.7560\n",
            "Epoch [49400/100000], Loss:  0.5470\n",
            "Epoch [49500/100000], Loss:  0.3552\n",
            "Epoch [49600/100000], Loss:  1.5435\n",
            "Epoch [49700/100000], Loss:  0.6384\n",
            "Epoch [49800/100000], Loss:  2.8168\n",
            "Epoch [49900/100000], Loss:  0.3231\n",
            "Epoch [50000/100000], Loss:  0.4986\n",
            "Epoch [50100/100000], Loss:  0.7121\n",
            "Epoch [50200/100000], Loss:  0.6113\n",
            "Epoch [50300/100000], Loss:  0.3564\n",
            "Epoch [50400/100000], Loss:  1.2040\n",
            "Epoch [50500/100000], Loss:  0.7467\n",
            "Epoch [50600/100000], Loss:  0.8881\n",
            "Epoch [50700/100000], Loss:  0.2798\n",
            "Epoch [50800/100000], Loss:  0.8346\n",
            "Epoch [50900/100000], Loss:  0.5130\n",
            "Epoch [51000/100000], Loss:  0.4023\n",
            "Epoch [51100/100000], Loss:  0.4982\n",
            "Epoch [51200/100000], Loss:  0.7482\n",
            "Epoch [51300/100000], Loss:  0.3233\n",
            "Epoch [51400/100000], Loss:  0.8500\n",
            "Epoch [51500/100000], Loss:  0.5832\n",
            "Epoch [51600/100000], Loss:  1.0967\n",
            "Epoch [51700/100000], Loss:  0.5994\n",
            "Epoch [51800/100000], Loss:  0.2878\n",
            "Epoch [51900/100000], Loss:  0.5937\n",
            "Epoch [52000/100000], Loss:  0.3880\n",
            "Epoch [52100/100000], Loss:  0.5214\n",
            "Epoch [52200/100000], Loss:  0.8182\n",
            "Epoch [52300/100000], Loss:  0.3637\n",
            "Epoch [52400/100000], Loss:  0.6569\n",
            "Epoch [52500/100000], Loss:  0.5706\n",
            "Epoch [52600/100000], Loss:  1.2807\n",
            "Epoch [52700/100000], Loss:  0.5780\n",
            "Epoch [52800/100000], Loss:  0.3112\n",
            "Epoch [52900/100000], Loss:  0.4697\n",
            "Epoch [53000/100000], Loss:  0.3889\n",
            "Epoch [53100/100000], Loss:  0.7864\n",
            "Epoch [53200/100000], Loss:  0.5374\n",
            "Epoch [53300/100000], Loss:  0.3422\n",
            "Epoch [53400/100000], Loss:  1.5699\n",
            "Epoch [53500/100000], Loss:  0.6297\n",
            "Epoch [53600/100000], Loss:  2.5319\n",
            "Epoch [53700/100000], Loss:  0.2926\n",
            "Epoch [53800/100000], Loss:  0.5139\n",
            "Epoch [53900/100000], Loss:  0.6911\n",
            "Epoch [54000/100000], Loss:  0.3791\n",
            "Epoch [54100/100000], Loss:  0.5311\n",
            "Epoch [54200/100000], Loss:  0.5825\n",
            "Epoch [54300/100000], Loss:  0.4148\n",
            "Epoch [54400/100000], Loss:  1.2389\n",
            "Epoch [54500/100000], Loss:  0.3169\n",
            "Epoch [54600/100000], Loss:  0.5235\n",
            "Epoch [54700/100000], Loss:  1.4641\n",
            "Epoch [54800/100000], Loss:  0.5368\n",
            "Epoch [54900/100000], Loss:  0.6053\n",
            "Epoch [55000/100000], Loss:  0.2674\n",
            "Epoch [55100/100000], Loss:  0.3945\n",
            "Epoch [55200/100000], Loss:  0.3374\n",
            "Epoch [55300/100000], Loss:  0.7093\n",
            "Epoch [55400/100000], Loss:  0.4954\n",
            "Epoch [55500/100000], Loss:  1.1928\n",
            "Epoch [55600/100000], Loss:  0.5767\n",
            "Epoch [55700/100000], Loss:  0.3203\n",
            "Epoch [55800/100000], Loss:  0.3353\n",
            "Epoch [55900/100000], Loss:  0.3525\n",
            "Epoch [56000/100000], Loss:  0.7940\n",
            "Epoch [56100/100000], Loss:  0.4357\n",
            "Epoch [56200/100000], Loss:  0.3584\n",
            "Epoch [56300/100000], Loss:  0.6752\n",
            "Epoch [56400/100000], Loss:  0.3150\n",
            "Epoch [56500/100000], Loss:  1.6033\n",
            "Epoch [56600/100000], Loss:  0.6043\n",
            "Epoch [56700/100000], Loss:  0.7350\n",
            "Epoch [56800/100000], Loss:  0.2429\n",
            "Epoch [56900/100000], Loss:  0.5793\n",
            "Epoch [57000/100000], Loss:  0.3204\n",
            "Epoch [57100/100000], Loss:  0.5235\n",
            "Epoch [57200/100000], Loss:  0.6336\n",
            "Epoch [57300/100000], Loss:  0.2744\n",
            "Epoch [57400/100000], Loss:  0.9794\n",
            "Epoch [57500/100000], Loss:  0.4178\n",
            "Epoch [57600/100000], Loss:  1.3324\n",
            "Epoch [57700/100000], Loss:  0.5182\n",
            "Epoch [57800/100000], Loss:  3.2653\n",
            "Epoch [57900/100000], Loss:  0.2438\n",
            "Epoch [58000/100000], Loss:  0.7295\n",
            "Epoch [58100/100000], Loss:  0.4489\n",
            "Epoch [58200/100000], Loss:  0.3185\n",
            "Epoch [58300/100000], Loss:  1.0702\n",
            "Epoch [58400/100000], Loss:  0.3331\n",
            "Epoch [58500/100000], Loss:  0.4127\n",
            "Epoch [58600/100000], Loss:  0.6580\n",
            "Epoch [58700/100000], Loss:  0.9052\n",
            "Epoch [58800/100000], Loss:  0.5242\n",
            "Epoch [58900/100000], Loss:  0.2714\n",
            "Epoch [59000/100000], Loss:  0.3176\n",
            "Epoch [59100/100000], Loss:  0.3197\n",
            "Epoch [59200/100000], Loss:  0.7489\n",
            "Epoch [59300/100000], Loss:  0.4439\n",
            "Epoch [59400/100000], Loss:  0.2862\n",
            "Epoch [59500/100000], Loss:  0.8267\n",
            "Epoch [59600/100000], Loss:  0.2946\n",
            "Epoch [59700/100000], Loss:  1.3861\n",
            "Epoch [59800/100000], Loss:  0.6689\n",
            "Epoch [59900/100000], Loss:  0.5405\n",
            "Epoch [60000/100000], Loss:  0.4760\n",
            "Epoch [60100/100000], Loss:  0.2616\n",
            "Epoch [60200/100000], Loss:  0.4258\n",
            "Epoch [60300/100000], Loss:  0.5730\n",
            "Epoch [60400/100000], Loss:  0.3313\n",
            "Epoch [60500/100000], Loss:  0.6160\n",
            "Epoch [60600/100000], Loss:  0.4291\n",
            "Epoch [60700/100000], Loss:  0.2756\n",
            "Epoch [60800/100000], Loss:  1.4023\n",
            "Epoch [60900/100000], Loss:  0.6530\n",
            "Epoch [61000/100000], Loss:  0.5228\n",
            "Epoch [61100/100000], Loss:  0.6000\n",
            "Epoch [61200/100000], Loss:  2.5625\n",
            "Epoch [61300/100000], Loss:  0.2589\n",
            "Epoch [61400/100000], Loss:  0.4898\n",
            "Epoch [61500/100000], Loss:  0.6324\n",
            "Epoch [61600/100000], Loss:  0.2467\n",
            "Epoch [61700/100000], Loss:  0.4072\n",
            "Epoch [61800/100000], Loss:  0.3168\n",
            "Epoch [61900/100000], Loss:  0.5382\n",
            "Epoch [62000/100000], Loss:  0.6217\n",
            "Epoch [62100/100000], Loss:  0.2887\n",
            "Epoch [62200/100000], Loss:  0.6615\n",
            "Epoch [62300/100000], Loss:  0.4159\n",
            "Epoch [62400/100000], Loss:  0.3138\n",
            "Epoch [62500/100000], Loss:  0.6899\n",
            "Epoch [62600/100000], Loss:  0.2867\n",
            "Epoch [62700/100000], Loss:  1.9386\n",
            "Epoch [62800/100000], Loss:  0.5430\n",
            "Epoch [62900/100000], Loss:  0.5234\n",
            "Epoch [63000/100000], Loss:  0.6297\n",
            "Epoch [63100/100000], Loss:  0.4925\n",
            "Epoch [63200/100000], Loss:  0.2324\n",
            "Epoch [63300/100000], Loss:  0.4203\n",
            "Epoch [63400/100000], Loss:  0.6272\n",
            "Epoch [63500/100000], Loss:  0.3022\n",
            "Epoch [63600/100000], Loss:  0.3854\n",
            "Epoch [63700/100000], Loss:  0.6426\n",
            "Epoch [63800/100000], Loss:  0.2807\n",
            "Epoch [63900/100000], Loss:  0.9208\n",
            "Epoch [64000/100000], Loss:  0.2838\n",
            "Epoch [64100/100000], Loss:  0.3713\n",
            "Epoch [64200/100000], Loss:  1.8735\n",
            "Epoch [64300/100000], Loss:  0.4922\n",
            "Epoch [64400/100000], Loss:  2.0069\n",
            "Epoch [64500/100000], Loss:  0.2279\n",
            "Epoch [64600/100000], Loss:  0.8178\n",
            "Epoch [64700/100000], Loss:  0.3807\n",
            "Epoch [64800/100000], Loss:  0.3010\n",
            "Epoch [64900/100000], Loss:  0.6115\n",
            "Epoch [65000/100000], Loss:  0.2180\n",
            "Epoch [65100/100000], Loss:  0.4291\n",
            "Epoch [65200/100000], Loss:  0.5316\n",
            "Epoch [65300/100000], Loss:  0.9808\n",
            "Epoch [65400/100000], Loss:  0.4752\n",
            "Epoch [65500/100000], Loss:  0.5426\n",
            "Epoch [65600/100000], Loss:  0.6529\n",
            "Epoch [65700/100000], Loss:  0.3013\n",
            "Epoch [65800/100000], Loss:  0.2658\n",
            "Epoch [65900/100000], Loss:  0.3049\n",
            "Epoch [66000/100000], Loss:  0.6226\n",
            "Epoch [66100/100000], Loss:  0.3080\n",
            "Epoch [66200/100000], Loss:  0.3134\n",
            "Epoch [66300/100000], Loss:  0.8184\n",
            "Epoch [66400/100000], Loss:  0.2952\n",
            "Epoch [66500/100000], Loss:  0.3650\n",
            "Epoch [66600/100000], Loss:  0.5223\n",
            "Epoch [66700/100000], Loss:  0.2434\n",
            "Epoch [66800/100000], Loss:  0.9084\n",
            "Epoch [66900/100000], Loss:  0.3140\n",
            "Epoch [67000/100000], Loss:  0.4057\n",
            "Epoch [67100/100000], Loss:  0.4999\n",
            "Epoch [67200/100000], Loss:  0.9645\n",
            "Epoch [67300/100000], Loss:  0.4152\n",
            "Epoch [67400/100000], Loss:  1.5572\n",
            "Epoch [67500/100000], Loss:  0.3096\n",
            "Epoch [67600/100000], Loss:  0.7228\n",
            "Epoch [67700/100000], Loss:  0.5782\n",
            "Epoch [67800/100000], Loss:  0.5219\n",
            "Epoch [67900/100000], Loss:  4.0321\n",
            "Epoch [68000/100000], Loss:  0.1969\n",
            "Epoch [68100/100000], Loss:  0.5624\n",
            "Epoch [68200/100000], Loss:  0.3594\n",
            "Epoch [68300/100000], Loss:  0.2952\n",
            "Epoch [68400/100000], Loss:  0.7198\n",
            "Epoch [68500/100000], Loss:  0.3753\n",
            "Epoch [68600/100000], Loss:  0.2410\n",
            "Epoch [68700/100000], Loss:  0.7309\n",
            "Epoch [68800/100000], Loss:  0.2235\n",
            "Epoch [68900/100000], Loss:  0.7510\n",
            "Epoch [69000/100000], Loss:  0.3523\n",
            "Epoch [69100/100000], Loss:  0.2931\n",
            "Epoch [69200/100000], Loss:  1.6806\n",
            "Epoch [69300/100000], Loss:  0.5426\n",
            "Epoch [69400/100000], Loss:  0.4577\n",
            "Epoch [69500/100000], Loss:  0.5936\n",
            "Epoch [69600/100000], Loss:  0.3154\n",
            "Epoch [69700/100000], Loss:  0.2202\n",
            "Epoch [69800/100000], Loss:  0.4808\n",
            "Epoch [69900/100000], Loss:  0.4953\n",
            "Epoch [70000/100000], Loss:  0.2911\n",
            "Epoch [70100/100000], Loss:  0.5353\n",
            "Epoch [70200/100000], Loss:  0.3517\n",
            "Epoch [70300/100000], Loss:  0.2442\n",
            "Epoch [70400/100000], Loss:  0.6501\n",
            "Epoch [70500/100000], Loss:  0.8077\n",
            "Epoch [70600/100000], Loss:  0.4574\n",
            "Epoch [70700/100000], Loss:  0.5333\n",
            "Epoch [70800/100000], Loss:  3.0521\n",
            "Epoch [70900/100000], Loss:  0.1929\n",
            "Epoch [71000/100000], Loss:  0.6057\n",
            "Epoch [71100/100000], Loss:  0.3106\n",
            "Epoch [71200/100000], Loss:  0.3047\n",
            "Epoch [71300/100000], Loss:  0.7195\n",
            "Epoch [71400/100000], Loss:  0.2228\n",
            "Epoch [71500/100000], Loss:  0.8661\n",
            "Epoch [71600/100000], Loss:  0.2812\n",
            "Epoch [71700/100000], Loss:  0.3203\n",
            "Epoch [71800/100000], Loss:  0.4927\n",
            "Epoch [71900/100000], Loss:  0.4211\n",
            "Epoch [72000/100000], Loss:  1.0092\n",
            "Epoch [72100/100000], Loss:  0.4748\n",
            "Epoch [72200/100000], Loss:  0.5578\n",
            "Epoch [72300/100000], Loss:  1.0795\n",
            "Epoch [72400/100000], Loss:  0.1869\n",
            "Epoch [72500/100000], Loss:  0.3856\n",
            "Epoch [72600/100000], Loss:  0.2569\n",
            "Epoch [72700/100000], Loss:  0.6227\n",
            "Epoch [72800/100000], Loss:  0.3678\n",
            "Epoch [72900/100000], Loss:  0.2332\n",
            "Epoch [73000/100000], Loss:  0.4961\n",
            "Epoch [73100/100000], Loss:  0.2189\n",
            "Epoch [73200/100000], Loss:  0.7694\n",
            "Epoch [73300/100000], Loss:  0.2312\n",
            "Epoch [73400/100000], Loss:  0.5737\n",
            "Epoch [73500/100000], Loss:  0.3016\n",
            "Epoch [73600/100000], Loss:  0.2469\n",
            "Epoch [73700/100000], Loss:  0.6836\n",
            "Epoch [73800/100000], Loss:  1.5717\n",
            "Epoch [73900/100000], Loss:  0.5487\n",
            "Epoch [74000/100000], Loss:  0.4633\n",
            "Epoch [74100/100000], Loss:  0.6210\n",
            "Epoch [74200/100000], Loss:  0.5672\n",
            "Epoch [74300/100000], Loss:  0.5179\n",
            "Epoch [74400/100000], Loss:  0.4722\n",
            "Epoch [74500/100000], Loss:  0.1925\n",
            "Epoch [74600/100000], Loss:  0.5471\n",
            "Epoch [74700/100000], Loss:  0.3541\n",
            "Epoch [74800/100000], Loss:  0.2618\n",
            "Epoch [74900/100000], Loss:  0.5943\n",
            "Epoch [75000/100000], Loss:  0.1908\n",
            "Epoch [75100/100000], Loss:  0.6902\n",
            "Epoch [75200/100000], Loss:  0.2141\n",
            "Epoch [75300/100000], Loss:  0.8018\n",
            "Epoch [75400/100000], Loss:  0.2106\n",
            "Epoch [75500/100000], Loss:  0.4465\n",
            "Epoch [75600/100000], Loss:  1.2816\n",
            "Epoch [75700/100000], Loss:  0.4455\n",
            "Epoch [75800/100000], Loss:  0.4725\n",
            "Epoch [75900/100000], Loss:  0.5728\n",
            "Epoch [76000/100000], Loss:  0.3225\n",
            "Epoch [76100/100000], Loss:  0.2062\n",
            "Epoch [76200/100000], Loss:  0.6898\n",
            "Epoch [76300/100000], Loss:  0.2659\n",
            "Epoch [76400/100000], Loss:  0.5030\n",
            "Epoch [76500/100000], Loss:  0.3156\n",
            "Epoch [76600/100000], Loss:  0.4035\n",
            "Epoch [76700/100000], Loss:  0.2574\n",
            "Epoch [76800/100000], Loss:  0.3235\n",
            "Epoch [76900/100000], Loss:  0.7289\n",
            "Epoch [77000/100000], Loss:  0.7424\n",
            "Epoch [77100/100000], Loss:  0.4110\n",
            "Epoch [77200/100000], Loss:  1.8338\n",
            "Epoch [77300/100000], Loss:  0.1839\n",
            "Epoch [77400/100000], Loss:  0.3703\n",
            "Epoch [77500/100000], Loss:  0.2967\n",
            "Epoch [77600/100000], Loss:  0.3035\n",
            "Epoch [77700/100000], Loss:  0.2563\n",
            "Epoch [77800/100000], Loss:  0.8444\n",
            "Epoch [77900/100000], Loss:  0.6512\n",
            "Epoch [78000/100000], Loss:  0.4324\n",
            "Epoch [78100/100000], Loss:  0.9006\n",
            "Epoch [78200/100000], Loss:  0.1771\n",
            "Epoch [78300/100000], Loss:  0.3112\n",
            "Epoch [78400/100000], Loss:  0.6342\n",
            "Epoch [78500/100000], Loss:  0.4767\n",
            "Epoch [78600/100000], Loss:  1.6280\n",
            "Epoch [78700/100000], Loss:  0.1751\n",
            "Epoch [78800/100000], Loss:  0.4333\n",
            "Epoch [78900/100000], Loss:  0.2332\n",
            "Epoch [79000/100000], Loss:  0.7216\n",
            "Epoch [79100/100000], Loss:  0.2655\n",
            "Epoch [79200/100000], Loss:  0.6207\n",
            "Epoch [79300/100000], Loss:  0.2291\n",
            "Epoch [79400/100000], Loss:  0.8858\n",
            "Epoch [79500/100000], Loss:  0.2145\n",
            "Epoch [79600/100000], Loss:  0.7901\n",
            "Epoch [79700/100000], Loss:  0.2466\n",
            "Epoch [79800/100000], Loss:  1.3913\n",
            "Epoch [79900/100000], Loss:  0.4476\n",
            "Epoch [80000/100000], Loss:  0.4539\n",
            "Epoch [80100/100000], Loss:  0.5628\n",
            "Epoch [80200/100000], Loss:  0.1791\n",
            "Epoch [80300/100000], Loss:  0.2222\n",
            "Epoch [80400/100000], Loss:  0.5324\n",
            "Epoch [80500/100000], Loss:  0.2952\n",
            "Epoch [80600/100000], Loss:  0.2833\n",
            "Epoch [80700/100000], Loss:  0.3973\n",
            "Epoch [80800/100000], Loss:  0.8146\n",
            "Epoch [80900/100000], Loss:  0.4434\n",
            "Epoch [81000/100000], Loss:  0.4325\n",
            "Epoch [81100/100000], Loss:  1.6372\n",
            "Epoch [81200/100000], Loss:  0.1718\n",
            "Epoch [81300/100000], Loss:  0.4297\n",
            "Epoch [81400/100000], Loss:  0.2363\n",
            "Epoch [81500/100000], Loss:  0.7820\n",
            "Epoch [81600/100000], Loss:  0.2259\n",
            "Epoch [81700/100000], Loss:  0.8696\n",
            "Epoch [81800/100000], Loss:  0.1716\n",
            "Epoch [81900/100000], Loss:  0.5338\n",
            "Epoch [82000/100000], Loss:  0.2099\n",
            "Epoch [82100/100000], Loss:  1.1981\n",
            "Epoch [82200/100000], Loss:  0.5557\n",
            "Epoch [82300/100000], Loss:  0.4204\n",
            "Epoch [82400/100000], Loss:  0.5073\n",
            "Epoch [82500/100000], Loss:  2.1086\n",
            "Epoch [82600/100000], Loss:  0.1673\n",
            "Epoch [82700/100000], Loss:  0.3515\n",
            "Epoch [82800/100000], Loss:  0.2163\n",
            "Epoch [82900/100000], Loss:  0.5513\n",
            "Epoch [83000/100000], Loss:  0.2148\n",
            "Epoch [83100/100000], Loss:  0.8402\n",
            "Epoch [83200/100000], Loss:  0.5857\n",
            "Epoch [83300/100000], Loss:  0.4252\n",
            "Epoch [83400/100000], Loss:  0.4389\n",
            "Epoch [83500/100000], Loss:  3.6618\n",
            "Epoch [83600/100000], Loss:  0.1594\n",
            "Epoch [83700/100000], Loss:  0.2917\n",
            "Epoch [83800/100000], Loss:  0.2586\n",
            "Epoch [83900/100000], Loss:  0.6566\n",
            "Epoch [84000/100000], Loss:  0.2295\n",
            "Epoch [84100/100000], Loss:  1.1425\n",
            "Epoch [84200/100000], Loss:  0.1727\n",
            "Epoch [84300/100000], Loss:  0.6367\n",
            "Epoch [84400/100000], Loss:  0.1932\n",
            "Epoch [84500/100000], Loss:  0.5194\n",
            "Epoch [84600/100000], Loss:  1.1228\n",
            "Epoch [84700/100000], Loss:  0.4279\n",
            "Epoch [84800/100000], Loss:  0.4344\n",
            "Epoch [84900/100000], Loss:  0.5533\n",
            "Epoch [85000/100000], Loss:  0.1613\n",
            "Epoch [85100/100000], Loss:  0.2013\n",
            "Epoch [85200/100000], Loss:  0.4445\n",
            "Epoch [85300/100000], Loss:  0.3063\n",
            "Epoch [85400/100000], Loss:  0.2407\n",
            "Epoch [85500/100000], Loss:  0.4658\n",
            "Epoch [85600/100000], Loss:  0.7298\n",
            "Epoch [85700/100000], Loss:  0.4195\n",
            "Epoch [85800/100000], Loss:  0.4296\n",
            "Epoch [85900/100000], Loss:  2.1679\n",
            "Epoch [86000/100000], Loss:  0.1540\n",
            "Epoch [86100/100000], Loss:  0.3943\n",
            "Epoch [86200/100000], Loss:  0.2268\n",
            "Epoch [86300/100000], Loss:  0.7195\n",
            "Epoch [86400/100000], Loss:  0.2188\n",
            "Epoch [86500/100000], Loss:  1.0514\n",
            "Epoch [86600/100000], Loss:  0.1662\n",
            "Epoch [86700/100000], Loss:  0.6547\n",
            "Epoch [86800/100000], Loss:  0.3432\n",
            "Epoch [86900/100000], Loss:  0.9750\n",
            "Epoch [87000/100000], Loss:  0.4053\n",
            "Epoch [87100/100000], Loss:  0.4339\n",
            "Epoch [87200/100000], Loss:  0.4968\n",
            "Epoch [87300/100000], Loss:  1.0032\n",
            "Epoch [87400/100000], Loss:  0.1528\n",
            "Epoch [87500/100000], Loss:  0.3151\n",
            "Epoch [87600/100000], Loss:  0.5118\n",
            "Epoch [87700/100000], Loss:  0.4391\n",
            "Epoch [87800/100000], Loss:  0.4306\n",
            "Epoch [87900/100000], Loss:  0.2737\n",
            "Epoch [88000/100000], Loss:  0.1572\n",
            "Epoch [88100/100000], Loss:  0.4861\n",
            "Epoch [88200/100000], Loss:  0.2647\n",
            "Epoch [88300/100000], Loss:  0.2721\n",
            "Epoch [88400/100000], Loss:  0.3692\n",
            "Epoch [88500/100000], Loss:  0.1837\n",
            "Epoch [88600/100000], Loss:  0.3179\n",
            "Epoch [88700/100000], Loss:  0.2121\n",
            "Epoch [88800/100000], Loss:  0.9034\n",
            "Epoch [88900/100000], Loss:  0.5844\n",
            "Epoch [89000/100000], Loss:  0.3912\n",
            "Epoch [89100/100000], Loss:  0.4481\n",
            "Epoch [89200/100000], Loss:  0.4676\n",
            "Epoch [89300/100000], Loss:  0.1762\n",
            "Epoch [89400/100000], Loss:  0.1604\n",
            "Epoch [89500/100000], Loss:  0.3568\n",
            "Epoch [89600/100000], Loss:  0.4919\n",
            "Epoch [89700/100000], Loss:  0.4118\n",
            "Epoch [89800/100000], Loss:  0.5015\n",
            "Epoch [89900/100000], Loss:  0.1541\n",
            "Epoch [90000/100000], Loss:  0.1912\n",
            "Epoch [90100/100000], Loss:  0.2999\n",
            "Epoch [90200/100000], Loss:  0.4109\n",
            "Epoch [90300/100000], Loss:  0.1984\n",
            "Epoch [90400/100000], Loss:  0.6638\n",
            "Epoch [90500/100000], Loss:  0.1589\n",
            "Epoch [90600/100000], Loss:  0.5021\n",
            "Epoch [90700/100000], Loss:  0.1748\n",
            "Epoch [90800/100000], Loss:  0.4608\n",
            "Epoch [90900/100000], Loss:  0.1816\n",
            "Epoch [91000/100000], Loss:  0.3782\n",
            "Epoch [91100/100000], Loss:  1.1525\n",
            "Epoch [91200/100000], Loss:  0.4272\n",
            "Epoch [91300/100000], Loss:  0.3952\n",
            "Epoch [91400/100000], Loss:  0.4537\n",
            "Epoch [91500/100000], Loss:  0.4765\n",
            "Epoch [91600/100000], Loss:  0.1792\n",
            "Epoch [91700/100000], Loss:  0.1574\n",
            "Epoch [91800/100000], Loss:  0.3351\n",
            "Epoch [91900/100000], Loss:  0.5001\n",
            "Epoch [92000/100000], Loss:  0.4235\n",
            "Epoch [92100/100000], Loss:  0.3951\n",
            "Epoch [92200/100000], Loss:  0.7399\n",
            "Epoch [92300/100000], Loss:  0.1461\n",
            "Epoch [92400/100000], Loss:  0.3852\n",
            "Epoch [92500/100000], Loss:  0.1973\n",
            "Epoch [92600/100000], Loss:  0.5535\n",
            "Epoch [92700/100000], Loss:  0.2248\n",
            "Epoch [92800/100000], Loss:  0.4425\n",
            "Epoch [92900/100000], Loss:  0.2056\n",
            "Epoch [93000/100000], Loss:  1.0477\n",
            "Epoch [93100/100000], Loss:  0.1523\n",
            "Epoch [93200/100000], Loss:  0.5760\n",
            "Epoch [93300/100000], Loss:  0.2130\n",
            "Epoch [93400/100000], Loss:  0.4899\n",
            "Epoch [93500/100000], Loss:  0.9611\n",
            "Epoch [93600/100000], Loss:  0.3891\n",
            "Epoch [93700/100000], Loss:  0.4012\n",
            "Epoch [93800/100000], Loss:  0.4635\n",
            "Epoch [93900/100000], Loss:  0.4232\n",
            "Epoch [94000/100000], Loss:  3.6267\n",
            "Epoch [94100/100000], Loss:  0.1407\n",
            "Epoch [94200/100000], Loss:  0.2011\n",
            "Epoch [94300/100000], Loss:  0.4417\n",
            "Epoch [94400/100000], Loss:  0.4430\n",
            "Epoch [94500/100000], Loss:  0.4122\n",
            "Epoch [94600/100000], Loss:  0.4099\n",
            "Epoch [94700/100000], Loss:  1.2834\n",
            "Epoch [94800/100000], Loss:  0.1417\n",
            "Epoch [94900/100000], Loss:  0.2407\n",
            "Epoch [95000/100000], Loss:  0.4710\n",
            "Epoch [95100/100000], Loss:  0.4206\n",
            "Epoch [95200/100000], Loss:  0.3650\n",
            "Epoch [95300/100000], Loss:  1.0462\n",
            "Epoch [95400/100000], Loss:  0.1405\n",
            "Epoch [95500/100000], Loss:  0.1768\n",
            "Epoch [95600/100000], Loss:  0.2957\n",
            "Epoch [95700/100000], Loss:  0.3770\n",
            "Epoch [95800/100000], Loss:  0.1914\n",
            "Epoch [95900/100000], Loss:  0.5636\n",
            "Epoch [96000/100000], Loss:  0.1540\n",
            "Epoch [96100/100000], Loss:  0.3930\n",
            "Epoch [96200/100000], Loss:  0.1653\n",
            "Epoch [96300/100000], Loss:  0.2829\n",
            "Epoch [96400/100000], Loss:  0.2212\n",
            "Epoch [96500/100000], Loss:  0.2906\n",
            "Epoch [96600/100000], Loss:  1.1095\n",
            "Epoch [96700/100000], Loss:  0.4075\n",
            "Epoch [96800/100000], Loss:  0.3727\n",
            "Epoch [96900/100000], Loss:  0.3469\n",
            "Epoch [97000/100000], Loss:  0.2671\n",
            "Epoch [97100/100000], Loss:  0.1752\n",
            "Epoch [97200/100000], Loss:  0.3939\n",
            "Epoch [97300/100000], Loss:  0.4638\n",
            "Epoch [97400/100000], Loss:  0.3949\n",
            "Epoch [97500/100000], Loss:  1.7909\n",
            "Epoch [97600/100000], Loss:  0.1381\n",
            "Epoch [97700/100000], Loss:  0.2200\n",
            "Epoch [97800/100000], Loss:  0.4822\n",
            "Epoch [97900/100000], Loss:  0.4248\n",
            "Epoch [98000/100000], Loss:  0.3942\n",
            "Epoch [98100/100000], Loss:  0.1852\n",
            "Epoch [98200/100000], Loss:  0.1471\n",
            "Epoch [98300/100000], Loss:  0.3458\n",
            "Epoch [98400/100000], Loss:  0.1593\n",
            "Epoch [98500/100000], Loss:  0.3902\n",
            "Epoch [98600/100000], Loss:  0.1813\n",
            "Epoch [98700/100000], Loss:  0.6088\n",
            "Epoch [98800/100000], Loss:  0.1446\n",
            "Epoch [98900/100000], Loss:  0.3481\n",
            "Epoch [99000/100000], Loss:  0.1798\n",
            "Epoch [99100/100000], Loss:  0.2215\n",
            "Epoch [99200/100000], Loss:  0.2338\n",
            "Epoch [99300/100000], Loss:  0.3125\n",
            "Epoch [99400/100000], Loss:  0.9950\n",
            "Epoch [99500/100000], Loss:  0.3736\n",
            "Epoch [99600/100000], Loss:  0.3663\n",
            "Epoch [99700/100000], Loss:  1.1367\n",
            "Epoch [99800/100000], Loss:  0.1353\n",
            "Epoch [99900/100000], Loss:  0.1996\n",
            "Epoch [100000/100000], Loss:  0.5110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(100000),losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over itrations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "AlkYNmVi_6mw",
        "outputId": "46fde331-112a-4e90-971e-cf2cc1d1e4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "hjc8e31ZDHSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_test_pred = model(X_test)\n",
        "  test_loss = criterion(y_test_pred,y_test)\n",
        "  print(f'Test Loss: {test_loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFJKz5VqGYAS",
        "outputId": "5cd28d3b-48e1-4e7c-b3ab-77d78307904e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 9.5076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae = torch.abs(y_test_pred - y_test).mean()\n",
        "print('Mean Absolute Error:',mae.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKJkGACnH7cP",
        "outputId": "1aa387c0-43bb-44a1-9f0f-78214c9b1c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 2.3018765449523926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(y_true,y_prediction):\n",
        "  return 100 * torch.mean(torch.abs((y_true-y_prediction) / y_true))\n",
        "\n",
        "mape = mean_absolute_percentage_error(y_test,y_test_pred)\n",
        "print(mape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_x9sa4IVxR",
        "outputId": "2d71285c-5874-411b-a089-c5997e2ed4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.7347)\n"
          ]
        }
      ]
    }
  ]
}